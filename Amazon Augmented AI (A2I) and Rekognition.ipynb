{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Augmented AI (A2I) - Amazon Rekognition example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Introduction](#Introduction)\n",
    "2. [Prerequisites](#Prerequisites)\n",
    "    1. [Supported Regions](#Supported-Regions)\n",
    "    2. [Workteam](#Workteam)\n",
    "    3. [Notebook Permission](#Notebook-Permission)\n",
    "    4. [Human Loop Role](#Human-Loop-Role)\n",
    "3. [Client Setup](#Client-Setup)\n",
    "4. [Sample Data](#Sample-Data)\n",
    "    1. [Download sample images](#Download-sample-images)\n",
    "    2. [Upload images to S3](#Upload-images-to-S3)\n",
    "5. [Create Control Plane Resources](#Create-Control-Plane-Resources)\n",
    "    1. [Create Human Task UI](#Create-Human-Task-UI)\n",
    "    2. [Create Flow Definition](#Create-Flow-Definition)\n",
    "6. [Start Human Loop](#Start-Human-Loop)\n",
    "    1. [Detect Moderation Labels with AWS Rekognition](#Detect-Moderation-Labels-with-AWS-Rekognition)\n",
    "    2. [When Human Loop Activation Conditions Are Met](#When-Human-Loop-Activation-Conditions-Are-Met)\n",
    "    3. [When Human Loop Activation Conditions Are Not Met](#When-Human-Loop-Activation-Conditions-Are-Not-Met)\n",
    "7. [Monitoring Human Loop](#Monitoring-Human-Loop)\n",
    "8. [Checking Task Results](#Checking-Task-Results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "In this tutorial, we will be showing you how you can use Amazon Augmented AI (A2I) directly with your calls to Rekognition's Detect Moderation Labels  API. \n",
    "\n",
    "For more in depth instructions, visit https://docs.aws.amazon.com/sagemaker/latest/dg/a2i-getting-started.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To incorporate Amazon A2I into your data labeling workflow for all task types, you need three resources:\n",
    "\n",
    "* A **worker task template** to create a worker UI. The worker UI displays your input data, such as documents or images, and instructions to workers. It also provides interactive tools that the worker uses to complete your tasks. For more information, see Create a Worker UI.\n",
    "\n",
    "* A **human review workflow**, also referred to as a flow definition. You use the flow definition to configure your human workforce and provide information about how to accomplish the labeling task. For built-in task types, you also use the flow definition to identify the conditions under which a review human loop is triggered. For example, Amazon Rekognition can perform image content moderation using machine learning. You can use the flow definition to specify that an image will be sent to a human for content moderation review if Amazon Rekognition's confidence is too low. You can create a flow definition in the Amazon SageMaker console or with the Amazon SageMaker API. To learn more about both of these options, see Create a Flow Definition.\n",
    "\n",
    "* A **human loop** to start your human review workflow. When you use one of the built-in task types, the corresponding AWS service creates and starts a human loop on your behalf when the conditions specified in your flow definition are met or for each object if no conditions were specified. When a human loop is triggered, human review tasks are sent to the workers as specified in the flow definition.\n",
    "\n",
    "When using a custom task type, you start a human loop using the Amazon Augmented AI Runtime API. When you call StartHumanLoop in your custom application, a task is sent to human reviewers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supported Regions\n",
    "\n",
    "For the A2I Preview, the **us-east-1** AWS region is supported. Please ensure that any accompanying resources are created in that region, including S3 buckets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workteam or Workforce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A workforce is the group of workers that you have selected to label your dataset. You can choose either the Amazon Mechanical Turk workforce, a vendor-managed workforce, or you can create your own private workforce to label or review your dataset. Whichever workforce type you choose, Amazon SageMaker takes care of sending tasks to workers. \n",
    "\n",
    "When you use a private workforce, you also create work teams, a group of workers from your workforce that are assigned to specific jobs— Amazon SageMaker Ground Truth labeling jobs or Amazon Augmented AI human review tasks. You can have multiple work teams and can assign one or more work teams to each job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create your Workteam, visit the instructions here: https://docs.aws.amazon.com/sagemaker/latest/dg/sms-workforce-management.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKTEAM_ARN= \"<YOUR_WORKTEAM_ARN>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook Permission\n",
    "\n",
    "The AWS IAM Role used to execute the notebook needs to have the following permissions:\n",
    "\n",
    "* RekognitionFullAccess\n",
    "* SagemakerFullAccess\n",
    "* S3 Read Access to the bucket listed below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arn:aws:iam::053520186210:role/service-role/AmazonSageMaker-ExecutionRole-20191231T143745'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sagemaker import get_execution_role\n",
    "\n",
    "# Setting Role to the default SageMaker Execution Role\n",
    "ROLE = get_execution_role()\n",
    "display(ROLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visit: https://docs.aws.amazon.com/sagemaker/latest/dg/a2i-permissions-security.html to add the necessary permissions to your role"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "We need to set up the following data:\n",
    "* `region` - Region to call A2I. Must be us-east-1 for Preview\n",
    "* `bucket` - A S3 bucket accessible by the given role\n",
    "    * Used to store the sample images & output results\n",
    "    * Must be within the same region as the region\n",
    "* `role` - The IAM role used as part of StartHumanLoop. By default, this notebook will use the execution role\n",
    "* `workteam` - The Amazon SageMaker Workteam to send the work to. Defaults to the first workteam in the account"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Client Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are going to setup the clients. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import io\n",
    "import json\n",
    "import uuid\n",
    "import botocore\n",
    "import time\n",
    "import botocore\n",
    "\n",
    "# Your bucket\n",
    "BUCKET = '<YOUR_BUCKET>'\n",
    "\n",
    "# Where your reuslts will be written to. \n",
    "OUTPUT_PATH = f's3://{BUCKET}/a2i-results'\n",
    "\n",
    "REGION = 'us-east-1'\n",
    "\n",
    "# Amazon SageMaker client\n",
    "sagemaker = boto3.client('sagemaker', REGION)\n",
    "\n",
    "# Amazon Rekognition client\n",
    "rekognition = boto3.client('rekognition', REGION)\n",
    "\n",
    "# S3 client\n",
    "s3 = boto3.client('s3', REGION)\n",
    "\n",
    "# Flow definition name - this value is unique per account and region. You can also provide your own value here.\n",
    "flowDefinitionName = 'fd-rekognition-demo' \n",
    "\n",
    "# Task UI name - this value is unique per account and region. You can also provide your own value here.\n",
    "taskUIName = 'ui-rekognition-demo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "# Pretty print setup\n",
    "pp = pprint.PrettyPrinter(indent=2)\n",
    "\n",
    "# Function to pretty-print AWS SDK responses\n",
    "def print_response(response):\n",
    "    if 'ResponseMetadata' in response:\n",
    "        del response['ResponseMetadata']\n",
    "    pp.pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Data\n",
    "\n",
    "We'll be using sample images from AWS Rekognition image moderation console.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download sample images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-03-19 01:43:56--  https://dhei5unw3vrsx.cloudfront.net/images/family_picnic_resized.jpg\n",
      "Resolving dhei5unw3vrsx.cloudfront.net (dhei5unw3vrsx.cloudfront.net)... 52.85.145.54, 52.85.145.122, 52.85.145.175, ...\n",
      "Connecting to dhei5unw3vrsx.cloudfront.net (dhei5unw3vrsx.cloudfront.net)|52.85.145.54|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1009903 (986K) [image/jpeg]\n",
      "Saving to: ‘family_picnic_resized.jpg’\n",
      "\n",
      "family_picnic_resiz 100%[===================>] 986.23K  --.-KB/s    in 0.02s   \n",
      "\n",
      "2020-03-19 01:43:57 (62.0 MB/s) - ‘family_picnic_resized.jpg’ saved [1009903/1009903]\n",
      "\n",
      "--2020-03-19 01:43:57--  https://dhei5unw3vrsx.cloudfront.net/images/yoga_swimwear_resized.jpg\n",
      "Resolving dhei5unw3vrsx.cloudfront.net (dhei5unw3vrsx.cloudfront.net)... 52.85.145.213, 52.85.145.54, 52.85.145.122, ...\n",
      "Connecting to dhei5unw3vrsx.cloudfront.net (dhei5unw3vrsx.cloudfront.net)|52.85.145.213|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1115181 (1.1M) [image/jpeg]\n",
      "Saving to: ‘yoga_swimwear_resized.jpg’\n",
      "\n",
      "yoga_swimwear_resiz 100%[===================>]   1.06M  --.-KB/s    in 0.03s   \n",
      "\n",
      "2020-03-19 01:43:57 (31.4 MB/s) - ‘yoga_swimwear_resized.jpg’ saved [1115181/1115181]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download images\n",
    "!wget 'https://dhei5unw3vrsx.cloudfront.net/images/family_picnic_resized.jpg' -O 'family_picnic_resized.jpg'\n",
    "!wget 'https://dhei5unw3vrsx.cloudfront.net/images/yoga_swimwear_resized.jpg' -O 'yoga_swimwear_resized.jpg'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yoga swimwear <img src=\"yoga_swimwear_resized.jpg\" alt=\"Yoga Swimwear\" width=\"400\" />\n",
    "Family picnic <img src=\"family_picnic_resized.jpg\" alt=\"Family picnic\" width=\"400\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload images to S3\n",
    "\n",
    "Upload the sample images to your S3 bucket. They will be read by AWS Rekognition and by A2I if human task is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "imageName1 = 'yoga_swimwear_resized.jpg'\n",
    "imageName2 = 'family_picnic_resized.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3.upload_file(imageName1, BUCKET, imageName1)\n",
    "s3.upload_file(imageName2, BUCKET, imageName2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now check the S3 bucket BUCKET that it contains the images at the specified key paths."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Control Plane Resources\n",
    "\n",
    "\n",
    "Here we'll be constructing the following control plane resources: task UI and flow definition, using the CreateTaskUI and CreateFlowDefinition APIs, respectively.\n",
    "\n",
    "These resources can be created once and used to drive any subsequent A2I human loops."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Human Task UI\n",
    "\n",
    "Create a human task UI resource, giving a UI template in liquid html. This template will be rendered to the a human workers whenever human loop is required.\n",
    "\n",
    "We are providing a simple demo template that is compatible with AWS Rekognition moderation label input and inferences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = r\"\"\"\n",
    "<script src=\"https://assets.crowd.aws/crowd-html-elements.js\"></script>\n",
    "{% capture s3_arn %}http://s3.amazonaws.com/{{ task.input.aiServiceRequest.image.s3Object.bucket }}/{{ task.input.aiServiceRequest.image.s3Object.name }}{% endcapture %}\n",
    "\n",
    "<crowd-form>\n",
    "  <crowd-rekognition-detect-moderation-labels\n",
    "    categories='[\n",
    "      {% for label in task.input.selectedAiServiceResponse.moderationLabels %}\n",
    "        {\n",
    "          name: \"{{ label.name }}\",\n",
    "          parentName: \"{{ label.parentName }}\",\n",
    "        },\n",
    "      {% endfor %}\n",
    "    ]'\n",
    "    src=\"{{ s3_arn | grant_read_access }}\"\n",
    "    header=\"Please select all categories that apply\"\n",
    "  >\n",
    "    <short-instructions header=\"Instructions\">\n",
    "      <style>\n",
    "        .instructions {\n",
    "          white-space: pre-wrap;\n",
    "        }\n",
    "      </style>\n",
    "      <p class='instructions'>Review the image and choose all applicable categories.\n",
    "        If no categories apply, choose None.\n",
    "\n",
    "        <b>Nudity</b>\n",
    "        Visuals depicting nude male or female person or persons\n",
    "\n",
    "        <b>Graphic Male Nudity</b>\n",
    "        Visuals depicting full frontal male nudity, often close ups\n",
    "\n",
    "        <b>Graphic Female Nudity</b>\n",
    "        Visuals depicting full frontal female nudity, often close ups\n",
    "\n",
    "        <b>Sexual Activity</b>\n",
    "        Visuals depicting various types of explicit sexual activities and pornography\n",
    "\n",
    "        <b>Illustrated Nudity or Sexual Activity</b>\n",
    "        Visuals depicting animated or drawn sexual activity, nudity or pornography\n",
    "\n",
    "        <b>Adult Toys</b>\n",
    "        Visuals depicting adult toys, often in a marketing context\n",
    "\n",
    "        <b>Female Swimwear or Underwear</b>\n",
    "        Visuals depicting female person wearing only swimwear or underwear\n",
    "\n",
    "        <b>Male Swimwear Or Underwear</b>\n",
    "        Visuals depicting male person wearing only swimwear or underwear\n",
    "\n",
    "        <b>Partial Nudity</b>\n",
    "        Visuals depicting covered up nudity, for example using hands or pose\n",
    "\n",
    "        <b>Revealing Clothes</b>\n",
    "        Visuals depicting revealing clothes and poses, such as deep cut dresses\n",
    "\n",
    "        <b>Graphic Violence or Gore</b>\n",
    "        Visuals depicting prominent blood or bloody injuries\n",
    "\n",
    "        <b>Physical Violence</b>\n",
    "        Visuals depicting violent physical assault, such as kicking or punching\n",
    "\n",
    "        <b>Weapon Violence</b>\n",
    "        Visuals depicting violence using weapons like firearms or blades, such as shooting\n",
    "\n",
    "        <b>Weapons</b>\n",
    "        Visuals depicting weapons like firearms and blades\n",
    "\n",
    "        <b>Self Injury</b>\n",
    "        Visuals depicting self-inflicted cutting on the body, typically in distinctive patterns using sharp objects\n",
    "\n",
    "        <b>Emaciated Bodies</b>\n",
    "        Visuals depicting extremely malnourished human bodies\n",
    "\n",
    "        <b>Corpses</b>\n",
    "        Visuals depicting human dead bodies\n",
    "\n",
    "        <b>Hanging</b>\n",
    "        Visuals depicting death by hanging</p>\n",
    "    </short-instructions>\n",
    "\n",
    "    <full-instructions header=\"Instructions\"></full-instructions>\n",
    "  </crowd-rekognition-detect-moderation-labels>\n",
    "</crowd-form>\n",
    "\"\"\"\n",
    "\n",
    "def create_task_ui():\n",
    "    '''\n",
    "    Creates a Human Task UI resource.\n",
    "\n",
    "    Returns:\n",
    "    struct: HumanTaskUiArn\n",
    "    '''\n",
    "    response = sagemaker.create_human_task_ui(\n",
    "        HumanTaskUiName=taskUIName,\n",
    "        UiTemplate={'Content': template})\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:sagemaker:us-east-1:053520186210:human-task-ui/ui-rekognition-demo\n"
     ]
    }
   ],
   "source": [
    "# Create task UI\n",
    "humanTaskUiResponse = create_task_ui()\n",
    "humanTaskUiArn = humanTaskUiResponse['HumanTaskUiArn']\n",
    "print(humanTaskUiArn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Flow Definition\n",
    "\n",
    "Now we can create a Flow Definition. The Flow Definition encapsulates the following high-level concepts:\n",
    "\n",
    "* The AWS managed request source (such as AWS Rekognition content moderation **AWS/Rekognition/Image/ContentModeration/V3**)\n",
    "* Conditions under which human loop is created, based on the inference result\n",
    "* Workteam to process the human tasks, number of workers per task etc\n",
    "* Task UI template to use\n",
    "* Output S3 location for the human task results\n",
    "\n",
    "Flow Definition is associated with a particular AWS managed request source, which affects the structure of the human loop activation conditions and format of the inference input and result.\n",
    "\n",
    "The human loop activation conditions used in this demo are tailored towards AWS Rekognition content moderation - they are based on the confidence thresholds for particular moderation labels.\n",
    "Activation conditions can be expressed using logical operators *And*, *Or*, *Not*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_flow_definition():\n",
    "    '''\n",
    "    Creates a Flow Definition resource\n",
    "\n",
    "    Returns:\n",
    "    struct: FlowDefinitionArn\n",
    "    '''\n",
    "    humanLoopActivationConditions = json.dumps(\n",
    "        {\n",
    "            \"Conditions\": [\n",
    "                {\n",
    "                  \"Or\": [\n",
    "                    {\n",
    "                        \"ConditionType\": \"ModerationLabelConfidenceCheck\",\n",
    "                        \"ConditionParameters\": {\n",
    "                            \"ModerationLabelName\": \"Suggestive\",\n",
    "                            \"ConfidenceLessThan\": 98\n",
    "                        }\n",
    "                    },\n",
    "                    {\n",
    "                        \"ConditionType\": \"ModerationLabelConfidenceCheck\",\n",
    "                        \"ConditionParameters\": {\n",
    "                            \"ModerationLabelName\": \"Female Swimwear Or Underwear\",\n",
    "                            \"ConfidenceGreaterThan\": 98\n",
    "                        }\n",
    "                    }\n",
    "                  ]\n",
    "               }\n",
    "            ]\n",
    "        }\n",
    "    )\n",
    "\n",
    "    response = sagemaker.create_flow_definition(\n",
    "            FlowDefinitionName= flowDefinitionName,\n",
    "            RoleArn= ROLE,\n",
    "            HumanLoopConfig= {\n",
    "                \"WorkteamArn\": WORKTEAM_ARN,\n",
    "                \"HumanTaskUiArn\": humanTaskUiArn,\n",
    "                \"TaskCount\": 1,\n",
    "                \"TaskDescription\": \"Demo A2I moderation sample task description\",\n",
    "                \"TaskTitle\": \"Demo A2I moderation sample task\"\n",
    "            },\n",
    "            HumanLoopActivationConfig={\n",
    "                \"HumanLoopRequestSource\": {\n",
    "                    \"AwsManagedHumanLoopRequestSource\": \"AWS/Rekognition/DetectModerationLabels/Image/V3\"\n",
    "                },\n",
    "                \"HumanLoopActivationConditionsConfig\": {\n",
    "                    \"HumanLoopActivationConditions\": humanLoopActivationConditions\n",
    "                }\n",
    "            },\n",
    "            OutputConfig={\n",
    "                \"S3OutputPath\" : OUTPUT_PATH\n",
    "            }\n",
    "        )\n",
    "    print_response(response)\n",
    "    return response['FlowDefinitionArn'] \n",
    "\n",
    "\n",
    "def describe_flow_definition(name):\n",
    "    '''\n",
    "    Describes Flow Definition\n",
    "\n",
    "    Returns:\n",
    "    struct: response from DescribeFlowDefinition API invocation\n",
    "    '''\n",
    "    return sagemaker.describe_flow_definition(\n",
    "        FlowDefinitionName=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ 'FlowDefinitionArn': 'arn:aws:sagemaker:us-east-1:053520186210:flow-definition/fd-rekognition-demo'}\n",
      "arn:aws:sagemaker:us-east-1:053520186210:flow-definition/fd-rekognition-demo\n"
     ]
    }
   ],
   "source": [
    "# Create a flow definition. We'll be using the flow definition arn for starting human loops.\n",
    "flow_definition_arn = create_flow_definition()\n",
    "print(flow_definition_arn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe flow definition - status should be active\n",
    "for x in range(60):\n",
    "    describeFlowDefinitionResponse = describe_flow_definition(flowDefinitionName)\n",
    "    print(describeFlowDefinitionResponse['FlowDefinitionStatus'])\n",
    "    if (describeFlowDefinitionResponse['FlowDefinitionStatus'] == 'Active'):\n",
    "        print(\"Flow Definition is active\")\n",
    "        break\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detect Moderation Labels with AWS Rekognition\n",
    "\n",
    "Let's call AWS Rekognition to detect moderation labels on the sample images stored in S3 based on the steps above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniqueId = str(uuid.uuid4())\n",
    "\n",
    "def detect_moderation_labels(img_name):\n",
    "    response = rekognition.detect_moderation_labels(Image={'S3Object': {'Bucket': BUCKET, 'Name': img_name}}, \n",
    "                                                    HumanLoopConfig={'HumanLoopName': uniqueId + '-1', 'FlowDefinitionArn': flow_definition_arn})\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When Human Loop Activation Conditions Are Met\n",
    "\n",
    "Image passed to Rekognition matches the conditions in FlowDefinition to involve Humans. So, HumanLoopArn will be present in the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first image has moderation labels, each one with a confidence score\n",
    "moderationResponse1 = detect_moderation_labels(imageName1)\n",
    "print_response(moderationResponse1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When Human Loop Activation Conditions Are Not Met\n",
    "\n",
    "Image passed to Rekognition does not match the conditions in FlowDefinition to involve Humans. SO, HumanLoopArn will not be present in the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first image has moderation labels, each one with a confidence score\n",
    "moderationResponse1 = detect_moderation_labels(imageName2)\n",
    "print_response(moderationResponse1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workteamName = WORKTEAM_ARN[WORKTEAM_ARN.rfind('/') + 1:]\n",
    "print(\"Navigate to the private worker portal and do the tasks\")\n",
    "print('https://' + sagemaker.describe_workteam(WorkteamName=workteamName)['Workteam']['SubDomain'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitoring Human Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A2I gives the user the ability to monitor the human loop until all the work has been completed by the selected workforce. Using the A2I runtime client, we can check on our human loop and get updates as fast as we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a2i_runtime_client = boto3.client('sagemaker-a2i-runtime', REGION)\n",
    "\n",
    "describe_human_loop_response = a2i_runtime_client.describe_human_loop(\n",
    "    HumanLoopName=uniqueId + '-1'\n",
    ")\n",
    "\n",
    "display(describe_human_loop_response['HumanLoopStatus'])\n",
    "display(describe_human_loop_response['HumanLoopOutput'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking Task Results\n",
    "\n",
    "Using your private workforce - go ahead and complete the tasks. The results should become available in the S3 OUTPUT_PATH when all work is completed.\n",
    "\n",
    "The exact path is\n",
    "`\n",
    "s3://<OUTPUT_PATH>/<flow-definition-name>/<Year>/<Month>/<Day>/<Minute>/<Hour>/<humanLoopName>/output.json\n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(describe_human_loop_response['HumanLoopOutput'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
